{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertForSequenceClassification, BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toxic.utils import (\n",
    "    perfect_bias,\n",
    "    TensorboardAggregator,\n",
    "    clip_to_max_len,\n",
    "    should_decay,\n",
    ")\n",
    "from toxic.metrics import IDENTITY_COLUMNS\n",
    "from toxic.bert import PipeLineConfig, convert_line_gpt, AUX_TARGETS\n",
    "from toxic.utils import seed_everything, convert_dataframe_to_bool\n",
    "from toxic.blocks.nn import GPT2CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "ACCUM_STEPS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpt(config: PipeLineConfig):\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    logging.info(\"Reading data...\")\n",
    "    input_folder = \"../input/jigsaw-unintended-bias-in-toxicity-classification/\"\n",
    "    train = pd.read_csv(os.path.join(input_folder, \"train.csv\"))\n",
    "\n",
    "    logging.info(\"Reading wiki PL...\")\n",
    "    wiki_sents = pd.read_csv(\"../input/wiki_sents.csv\")\n",
    "    wiki_subset = wiki_sents[\n",
    "        (wiki_sents.target < 0.1) & (wiki_sents[IDENTITY_COLUMNS].max(1) >= 0.33)\n",
    "    ].copy()\n",
    "    wiki_subset.drop(\n",
    "        columns=[\"any_identity\", \"max_identity\", \"target_aux\"], inplace=True\n",
    "    )\n",
    "    wiki_subset.iloc[:, :6] = 0.0  # They are not toxic by definition\n",
    "\n",
    "    logging.info(\"Sampling extra data...\")\n",
    "    seed_everything(config.seed + 1)\n",
    "    extras = []\n",
    "    t = convert_dataframe_to_bool(train)\n",
    "\n",
    "    for identity in IDENTITY_COLUMNS:\n",
    "        Ip = np.sum(t[identity] & t.target)\n",
    "        I = np.sum(t[identity])\n",
    "        Bp = np.sum(~t[identity] & t.target)\n",
    "        B = np.sum(~t[identity])\n",
    "        required = (Ip * B - Bp * I) // Bp\n",
    "\n",
    "        extra = wiki_subset[wiki_subset[identity] >= 0.333].copy()\n",
    "        logging.info(\"Mitigating bias for %s\", identity)\n",
    "        logging.info(\"Need %d extra samples, got %d\", required, len(extra))\n",
    "\n",
    "        if len(extra) > required:\n",
    "            logging.info(\"Downsampling extra dataframe\")\n",
    "            extra = extra.sample(required)\n",
    "        extras.append(extra)\n",
    "\n",
    "    enriched = pd.concat([train] + extras, ignore_index=True, sort=False, axis=0)\n",
    "\n",
    "    logging.info(\"Tokenizing...\")\n",
    "\n",
    "    with multiprocessing.Pool(processes=32) as pool:\n",
    "        text_list = enriched.comment_text.tolist()\n",
    "        sequences = pool.map(convert_line_gpt, text_list)\n",
    "\n",
    "    logging.info(\"Building ttensors for training...\")\n",
    "    sequences = np.array(sequences)\n",
    "    print(sequences.shape)\n",
    "    lengths = np.argmax(sequences == 0, axis=1)\n",
    "    lengths[lengths == 0] = sequences.shape[1]\n",
    "\n",
    "    logging.info(\"Bulding target tesnor...\")\n",
    "    iden = enriched[IDENTITY_COLUMNS].fillna(0).values\n",
    "    subgroup_target = np.hstack(\n",
    "        [\n",
    "            (iden >= 0.5).any(axis=1, keepdims=True).astype(np.int),\n",
    "            iden,\n",
    "            iden.max(axis=1, keepdims=True),\n",
    "        ]\n",
    "    )\n",
    "    sub_target_weigths = (\n",
    "        ~enriched[IDENTITY_COLUMNS].isna().values.any(axis=1, keepdims=True)\n",
    "    ).astype(np.int)\n",
    "\n",
    "    weights = np.ones(len(enriched))\n",
    "    weights += (iden >= 0.5).any(1)\n",
    "    weights += (enriched[\"target\"].values >= 0.5) & (iden < 0.5).any(1)\n",
    "    weights += (enriched[\"target\"].values < 0.5) & (iden >= 0.5).any(1)\n",
    "    weights /= weights.mean()\n",
    "\n",
    "    y_aux_train = enriched[AUX_TARGETS]\n",
    "    y_train_torch = torch.tensor(\n",
    "        np.hstack(\n",
    "            [\n",
    "                enriched.target.values[:, None],\n",
    "                weights[:, None],\n",
    "                y_aux_train,\n",
    "                subgroup_target,\n",
    "                sub_target_weigths,\n",
    "            ]\n",
    "        )\n",
    "    ).float()\n",
    "\n",
    "    logging.info(\"Seeding with seed %d ...\", config.seed)\n",
    "    seed_everything(config.seed)\n",
    "\n",
    "    logging.info(\"Creating dataset...\")\n",
    "    dataset = data.TensorDataset(\n",
    "        torch.tensor(sequences), y_train_torch, torch.tensor(lengths)\n",
    "    )\n",
    "    train_loader = data.DataLoader(\n",
    "        dataset, batch_size=BATCH_SIZE, collate_fn=clip_to_max_len, shuffle=True\n",
    "    )\n",
    "\n",
    "    logging.info(\"Creating a model...\")\n",
    "    model = GPT2CNN.from_pretrained(\"gpt2\", num_labels=18)\n",
    "    model.zero_grad()\n",
    "    model = model.cuda()\n",
    "\n",
    "    logs_file = f\"./tb_logs/final_{config.expname}\"\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if should_decay(n)],\n",
    "            \"weight_decay\": config.decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not should_decay(n)],\n",
    "            \"weight_decay\": 0.00,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = BertAdam(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=config.lr,\n",
    "        warmup=config.warmup,\n",
    "        t_total=config.epochs * len(train_loader) // ACCUM_STEPS,\n",
    "    )\n",
    "\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\", verbosity=0)\n",
    "    model = model.train()\n",
    "\n",
    "    writer = SummaryWriter(logs_file)\n",
    "    agg = TensorboardAggregator(writer)\n",
    "    custom_loss = prepare_loss(config)\n",
    "\n",
    "    for _ in range(config.epochs):\n",
    "        for j, (X, y) in enumerate(train_loader):\n",
    "\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            y_pred = model(X)\n",
    "            loss = custom_loss(y_pred, y)\n",
    "\n",
    "            accuracy = ((y_pred[:, 0] > 0) == (y[:, 0] > 0.5)).float().mean()\n",
    "            agg.log({\"train_loss\": loss.item(), \"train_accuracy\": accuracy.item()})\n",
    "\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "\n",
    "            if (j + 1) % ACCUM_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "    torch.save(model.state_dict(), f\"./models/final-pipe6-{config.expname}.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config_1 = PipeLineConfig(\n",
    "        expname=\"gpt_wiki_1\",\n",
    "        lr=4.9e-5,\n",
    "        warmup=0.06,\n",
    "        epochs=2,\n",
    "        seed=50462,\n",
    "        decay=0.04,\n",
    "        main_loss_weight=1.05,\n",
    "    )\n",
    "    config_2 = PipeLineConfig(\n",
    "        expname=\"gpt_wiki_2\",\n",
    "        lr=4.7e-5,\n",
    "        warmup=0.055,\n",
    "        epochs=2,\n",
    "        seed=54184,\n",
    "        decay=0.06,\n",
    "        main_loss_weight=0.98,\n",
    "    )\n",
    "\n",
    "    for config in (config_1, config_2):\n",
    "        train_gpt(config)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
